{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCBKNN-TFIDF.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"12sLbZZ0wkSoo-hgui1-m5x8Q6v-vIJIN","authorship_tag":"ABX9TyO6r92/LLESoPMsXuKjhp98"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtJ26eYaDTlC","executionInfo":{"status":"ok","timestamp":1657824729459,"user_tz":240,"elapsed":3652,"user":{"displayName":"Marcos Paulo Silva G么lo","userId":"00854347238887874462"}},"outputId":"0600de57-b782-4234-ce5f-64f9f7cb4685"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=18cij6foP2ilw2NtadINSnYP6UUbenXCf\n","To: /content/scc5966.zip\n","\r  0% 0.00/18.4M [00:00<?, ?B/s]\r100% 18.4M/18.4M [00:00<00:00, 197MB/s]\n"]}],"source":["!gdown  18cij6foP2ilw2NtadINSnYP6UUbenXCf"]},{"cell_type":"code","source":["!unzip scc5966.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFEFxaRgDcG6","executionInfo":{"status":"ok","timestamp":1657824730336,"user_tz":240,"elapsed":883,"user":{"displayName":"Marcos Paulo Silva G么lo","userId":"00854347238887874462"}},"outputId":"e8193379-2c07-4d0e-ec2c-7cd3b516e744"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  scc5966.zip\n","  inflating: movie_reviews.csv       \n","  inflating: movies_data.csv         \n","  inflating: test_data.csv           \n","  inflating: train_data .csv         \n","  inflating: users_data.csv          \n"]}]},{"cell_type":"code","source":["import pandas as pd \n","\n","df_movie_reviews = pd.read_csv('movie_reviews.csv')\n","\n","df_movies_data = pd.read_csv('movies_data.csv')\n","\n","df_test_data = pd.read_csv('test_data.csv')\n","\n","df_train_data = pd.read_csv('train_data .csv')\n","\n","df_users_data = pd.read_csv('users_data.csv')"],"metadata":{"id":"L9LfJnp2Dc97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm \n","\n","df_user_item = pd.DataFrame(index = df_train_data['user_id'].unique(), columns=df_train_data['movie_id'].unique())\n","\n","for movie in tqdm(df_train_data['movie_id'].unique()):\n","  temp = df_train_data[df_train_data.movie_id == movie]\n","  df_user_item.loc[temp.user_id, movie] = temp.rating.to_list()"],"metadata":{"id":"r5ejagQUDfiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_item_review = pd.DataFrame(index = df_train_data['movie_id'].unique(), columns=['reviews'])\n","\n","for movie in tqdm(df_train_data['movie_id'].unique()):\n","  temp = df_movie_reviews[df_movie_reviews.movie_id == movie]\n","  concated_reviews = ''\n","  for item,row in temp.iterrows():\n","    concated_reviews = concated_reviews + ' ' + row.text\n","  \n","  df_item_review.loc[movie]['reviews'] = concated_reviews"],"metadata":{"id":"VXAt7HV4Dto9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk.stem\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","def has_numbers(input_string):\n","    return any(char.isdigit() for char in input_string)\n","\n","def MyTokenizer(text):\n","  stopwords = nltk.corpus.stopwords.words('english')\n","  stemmer = nltk.stem.SnowballStemmer('english')\n","  wnl = WordNetLemmatizer()\n","  l1 = [t for t in word_tokenize(text)]\n","  l2 = []\n","  for token in l1:\n","    if token not in stopwords and token.isnumeric() is False and len(token) > 2 and has_numbers(token) is False:\n","      l2.append(token)\n","  l3 = [stemmer.stem(wnl.lemmatize(t)) for t in l2]\n","  return l3"],"metadata":{"id":"cVZf43IuIpCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import time\n","\n","start = time.time()\n","\n","TFIDF = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), min_df=1,max_features=10000, tokenizer=MyTokenizer)\n","sparse_tfidf = TFIDF.fit_transform(df_item_review['reviews'])\n","\n","end = time.time()\n","time_ = end - start\n","\n","print(\"TFIDF time: \" + str(time_))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1m7y5guICNI","executionInfo":{"status":"ok","timestamp":1656965572479,"user_tz":180,"elapsed":155513,"user":{"displayName":"Marcos Paulo Silva G么lo","userId":"00854347238887874462"}},"outputId":"4744ada2-3c45-4e31-ea2d-80c1de214dd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'le', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"output_type":"stream","name":"stdout","text":["TFIDF time: 155.371826171875\n"]}]},{"cell_type":"code","source":["df_tfidf = pd.DataFrame(sparse_tfidf.todense(), index = df_item_review.index, columns=TFIDF.get_feature_names())"],"metadata":{"id":"ZQXPa8eFGmWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity as cosine\n","import numpy as np"],"metadata":{"id":"sQnjzscZK0X3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cont = 1\n","dic_sim = {}\n","\n","for movie1 in tqdm(df_tfidf.index):\n","  for movie2 in df_tfidf.index[cont:]:\n","    x = np.array(df_tfidf.loc[movie1].to_list())\n","    y = np.array(df_tfidf.loc[movie2].to_list())\n","    \n","    similarity = cosine([x],[y])[0][0]\n","    \n","    dic_sim[(movie1,movie2)] = similarity\n","\n","  cont+=1"],"metadata":{"id":"zS5ipeaOLJja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open('/content/drive/MyDrive/USP/Doctorate/Credits/Recomender_Systems/similarities/total/dic_sim_items_tfidf_cosine.p', 'wb') as fp:\n","    pickle.dump(dic_sim, fp, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"id":"D8e-ttlURd8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open('/content/drive/MyDrive/USP/Doctorate/Credits/Recomender_Systems/similarities/total/dic_sim_items_tfidf_cosine.p', 'rb') as fp:\n","    data = pickle.load(fp)"],"metadata":{"id":"T91kLnMogkh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def similarity_itens(similarities, neighbor_item, item):\n","  \n","  if (item,neighbor_item) not in similarities.keys():\n","    similarity = similarities[(neighbor_item,item)]\n","  else:\n","    similarity = similarities[(item,neighbor_item)]\n","    \n","  return similarity\n","  \n","def k_neighbors_nearest(df, similarities, k, user, item):\n","  k_neighbors_similarity = [-2] * k\n","  k_neighbors = [-1] * k\n","\n","  for neighbor_item in df.columns:\n","    \n","    ni = neighbor_item\n","    if item != ni and df.loc[user][ni] is not np.nan:\n","      \n","      similarity = similarity_itens(similarities, ni, item)\n","\n","      for i in range(k):\n","        if similarity > k_neighbors_similarity[i]:\n","          aux = k_neighbors_similarity[i]\n","          k_neighbors_similarity[i] = similarity\n","          similarity = aux\n","\n","          aux = k_neighbors[i]\n","          k_neighbors[i] = ni\n","          ni = aux\n","\n","  return k_neighbors\n","\n","def pred(df, similarities, k, user, item):\n","\n","  k_neighbors = k_neighbors_nearest(df, similarities, k, user, item)\n","  \n","  sum = 0\n","  sum_similarity = 0\n","\n","  for neighbor_item in k_neighbors:\n","    if(neighbor_item != -1): #se nao deu o numero maximo de vizinhos mais proximos\n","      rating_neighbor_item = df.loc[user][neighbor_item]\n","    \n","      similarity = similarity_itens(similarities, neighbor_item, item)\n","\n","      sum_similarity+= similarity\n","      \n","      sum+= similarity * rating_neighbor_item\n","  \n","  if sum_similarity ==0 and sum ==0:\n","    return 0\n","  elif sum_similarity ==0 and sum !=0:\n","    return sum\n","  else:\n","    return sum/sum_similarity\n","\n","import numpy as np\n","\n","items_mean = np.mean(df_user_item)\n","user_mean = np.mean(df_user_item.T)\n","global_mean = np.nanmean(df_user_item.values.tolist())\n","\n","def pred_generic(parameters):\n","  return pred(parameters[0], parameters[1], parameters[2], parameters[3], parameters[4])\n","  \n","def generate(parameters):\n","  l= []\n","  \n","  for item,row in tqdm(df_test_data.iterrows()):\n","    user = row['user_id']\n","    item = row['movie_id']\n","    if user not in df_train_data['user_id'].unique() and item in df_train_data['movie_id'].unique():\n","      l.append(items_mean[item])\n","    elif user in df_train_data['user_id'].unique() and item not in df_train_data['movie_id'].unique():\n","      l.append(user_mean[user])\n","    elif user not in df_train_data['user_id'].unique() and item not in df_train_data['movie_id'].unique():\n","      l.append(global_mean)\n","    else:\n","      parameters.append(user)\n","      parameters.append(item)\n","\n","      value = pred_generic(parameters)\n","      if value < 1:\n","        value = 1\n","      if value > 5:\n","        value = 5\n","\n","      l.append(value)\n","  \n","  return l"],"metadata":{"id":"bA5xGe67DgBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k = 1\n","  \n","y_pred = generate([df_user_item, data, k])\n","  \n","df_test_data['rating'] = y_pred\n","df_rating = df_test_data[['id','rating']]\n","df_rating = df_rating.set_index('id')\n","name = 'FBC_TFIDF_' + str(k) + '.csv'\n","df_rating.to_csv('/content/drive/MyDrive/USP/Doctorate/Credits/Recomender_Systems/results-kaggle/' +name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwISv_CEDi7t","executionInfo":{"status":"ok","timestamp":1657825952437,"user_tz":240,"elapsed":1112343,"user":{"displayName":"Marcos Paulo Silva G么lo","userId":"00854347238887874462"}},"outputId":"013ed178-061a-4035-cb71-8e8e8e93d9da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["3970it [18:32,  3.57it/s]\n"]}]}]}